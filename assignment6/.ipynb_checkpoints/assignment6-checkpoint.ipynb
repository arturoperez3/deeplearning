{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Name: Arturo Perez \n",
    "    Email: arturo.e.perez@vanderbilt.edu\n",
    "    VUnet: perezae\n",
    "    Course: CS 3891 \n",
    "    Date: 22 March 2018 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Helper Functions\n",
    "We begin this assignment by defining the activation functions we will use later on. These include ReLU, Sigmoid, and their respective derivatives when applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#   ReLU activation function\n",
    "def ReLU (array):\n",
    "    return np.maximum(0, array)\n",
    "    \n",
    "#   derivative of ReLU activation function\n",
    "def dReLU(array):\n",
    "    array[array < 0] = 0\n",
    "    array[array > 0] = 1\n",
    "    return array\n",
    "\n",
    "#   Sigmoid activation function\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue by defining a checkAccuracy function that will calculate our accuracy and error rates of our learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#   compute the training and the test error\n",
    "def checkAccuracy(w, b , X, Y, T, neuralnetworkType) :\n",
    "    # initialize our paramters for use in forward propogation \n",
    "    z = [None] * len(w)\n",
    "    a = [None] * len(w)\n",
    "\n",
    "    # forward propopgation \n",
    "    # we use a[-1] (the a from the very last layer) to get our prediction labels\n",
    "    z[0] = w[0] @ X + b[0]\n",
    "    for i in range(len(w)-1):\n",
    "        a[i] = ReLU(z[i])\n",
    "        z[i+1] = w[i+1] @ a[i] + b[i+1]\n",
    "    a[-1] = sigmoid(z[-1])\n",
    "\n",
    "    # get the prediction labels  \n",
    "    result = np.where(a[-1] >= 0.5, 1.0, 0.0)\n",
    "\n",
    "    # compare our prediction labels to the actual labels (Y in this case)\n",
    "    dim = result.ndim\n",
    "    count = 0\n",
    "    if (dim == 1) :\n",
    "        for i in range(1, Y.size):\n",
    "            if result[i] == Y[0, i]:\n",
    "                count += 1\n",
    "    else :       \n",
    "        for i in range(1, Y.size):\n",
    "            if result[0, i] == Y[0, i]:\n",
    "                count += 1\n",
    "\n",
    "    # calculate accuracy and error rates \n",
    "    accuracy = (count/Y.shape[1]) * 100\n",
    "    error_rate = 100 - accuracy\n",
    "\n",
    "    # print results \n",
    "    print(T + \" accuracy rate for \" + neuralnetworkType + \" is : \" + str(accuracy) + \"%\")\n",
    "    print(T + \" error rate for \" + neuralnetworkType + \" is : \" + str(error_rate) + \"%\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing \n",
    "###### In this section we take care of preprocessing the MNIST data set.\n",
    "We start by reading in the MNIST data set. We seperate the training and testing data into 4 buffers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import struct\n",
    "import logging\n",
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist import MNIST\n",
    "\n",
    "\n",
    "# the training set is stored in this directory\n",
    "path = \"/Users/Arturo1/Desktop/Vanderbilt/2017-2018/Spring 2018/Deep Learning 3891/handwriting\"\n",
    "\n",
    "# load the MNIST data\n",
    "mndata = MNIST(path)\n",
    "train_images, train_labels = mndata.load_training() # training data\n",
    "test_images, test_labels = mndata.load_testing() # testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue by reshaping/rescaling the images and labels of both the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to (28x28,1) column vector and divide by 255\n",
    "training_size, test_size = 60000, 10000   \n",
    "\n",
    "train_images = np.array(train_images[:training_size], dtype=float).transpose() / 255\n",
    "train_labels = np.array(train_labels[:training_size], dtype=float)\n",
    "test_images = np.array(test_images[:test_size], dtype=float).transpose() / 255\n",
    "test_labels = np.array(test_labels[:test_size], dtype = float)\n",
    "train_labels = np.reshape(train_labels, (training_size, 1))\n",
    "test_labels = np.reshape(test_labels, (test_size, 1))\n",
    "\n",
    "# subset of 10,000 samples\n",
    "subsetX = train_images[:,10000:20000]\n",
    "subsetY = train_labels[10000:20000,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we want to predict whether a new image is equal to ID or not, we change the labels from train_labels and test_labels to be either 1 if they are equal to ID, or 0 if they are not. This will help us make our predictions later on and calculate the accuracy/error rates. In my case, ID is equal to 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# student ID\n",
    "ID = 6\n",
    "# change the labels to either 0 or 1\n",
    "def relabel(label):\n",
    "    for l in label:\n",
    "        if (l[0] == ID):\n",
    "            l[0] = 1\n",
    "        else:\n",
    "            l[0] = 0\n",
    "            \n",
    "relabel(train_labels)\n",
    "relabel(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Minibatch SGD \n",
    "Now we train a 3-layer neural to classify the data using minibatch SGD. We plot the cost function, as well as calculate the training and test error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNextMiniBatch(X, Y, batchSize) :\n",
    "    for i in range(0, X.shape[1], batchSize) :\n",
    "        yield (X[: , i:i + batchSize] , Y[:, i:i + batchSize ])\n",
    "\n",
    "def minibatchNeuralNetwork(p, q, alpha, batchSize, layer_list):\n",
    "    f, m = p.shape\n",
    "    \n",
    "    # initialize weights, biases and parameters \n",
    "    w = functools.reduce(lambda acc, neurons: (acc[0] + [np.random.randn(neurons, \n",
    "    acc[1]) * np.sqrt(2.0/acc[1])], neurons), layer_list, ([], f))[0]\n",
    "    b = [ np.random.randn(neurons, 1) * 0.01 for neurons in layer_list ]\n",
    "    z = [None] * len(w)\n",
    "    a = [None] * len(w)\n",
    "    costs = []\n",
    "\n",
    "    for i in range(0, 200):\n",
    "        # api function used to shuffle 2 numpy arrays in unison\n",
    "        p, q = shuffle_arrays_unison(arrays=[p.T, q.T], random_seed=3)\n",
    "        p = p.T\n",
    "        q = q.T\n",
    "\n",
    "        # for each minibatch: forward prop, compute cost, backward prop, update parameters\n",
    "        for X, Y in getNextMiniBatch(p, q, batchSize) :\n",
    "            # forward propopgation\n",
    "            z[0] = w[0] @ X + b[0]\n",
    "            for i in range(len(w)-1):\n",
    "                a[i] = ReLU(z[i])\n",
    "                z[i+1] = w[i+1] @ a[i] + b[i+1]\n",
    "            a[-1] = sigmoid(z[-1])\n",
    "\n",
    "            # compute cost\n",
    "            cost = - (Y @ np.log(a[-1].T) + (1 - Y) @ np.log(1 - a[-1].T))[0,0] / m\n",
    "            costs.append(cost)\n",
    "\n",
    "            dZ = a[-1] - Y\n",
    "            gradients = [] \n",
    "            # backward propogation \n",
    "            for k in range(len(z)-2, -1, -1):\n",
    "                # Calculate dW and DB\n",
    "                gradients.append([ (dZ @ a[k].T) / m, \n",
    "                    np.sum(dZ, axis=1).reshape(b[k+1].shape) / m ])\n",
    "            \n",
    "                dZ = (w[k+1].T @ dZ) * dReLU(z[k])\n",
    "\n",
    "            # Calculate dW and dB for the first layer\n",
    "            gradients.append([ (dZ @ X.T) / m, \n",
    "                np.sum(dZ, axis=1).reshape(b[0].shape) / m ])\n",
    "\n",
    "            # flip gradients so order is from first to last layer\n",
    "            gradients = gradients[::-1]\n",
    "\n",
    "            # update parameters \n",
    "            for j in range(len(gradients)):\n",
    "                w[j] = w[j] - (alpha * gradients[j][0])\n",
    "                b[j] = b[j] - (alpha * gradients[j][1])\n",
    "            \n",
    "    # plot cost function\n",
    "    plt.clf()\n",
    "    plt.plot(costs)\n",
    "    plt.title(\"MiniBatch Neural Network Cost Function (\" + str(m) + \" samples)\")\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    return(w, b)\n",
    " \n",
    "    \n",
    "\n",
    "layers = [20, 10]\n",
    "# learning rate = .1, minibatch size = 100\n",
    "w,b = minibatchNeuralNetwork(subsetX, subsetY.T, .1, 100, layers)\n",
    "neuralType = \"Minibatch Neural Network\"\n",
    "checkAccuracy(w, b, subsetX, subsetY.T, \"Train\", neuralType)\n",
    "checkAccuracy(w, b, test_images, test_labels, \"Test\", neuralType)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
