{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Arturo Perez \n",
    "##### Professor Koutsoukos \n",
    "##### CS 3891 Assignment 4\n",
    "##### 1 March 2018 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "###### In this section we take care of preprocessing the MNIST data set.\n",
    "We start by reading in the MNIST data set. We seperate the training and testing data into four buffers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import struct\n",
    "import logging\n",
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist import MNIST\n",
    "\n",
    "\n",
    "# the training set is stored in this directory\n",
    "path = \"/Users/Arturo1/Desktop/Vanderbilt/2017-2018/Spring 2018/Deep Learning 3891/handwriting\"\n",
    "\n",
    "# load the MNIST data\n",
    "mndata = MNIST(path)\n",
    "train_images, train_labels = mndata.load_training() # training data\n",
    "test_images, test_labels = mndata.load_testing() # testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue by reshaping/rescaling the images and labels of both the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to (28x28,1) column vector and divide by 255\n",
    "training_size, test_size = 60000, 10000   \n",
    "\n",
    "train_images = np.array(train_images[:training_size], dtype=float).transpose() / 255\n",
    "train_labels = np.array(train_labels[:training_size], dtype=float)\n",
    "test_images = np.array(test_images[:test_size], dtype=float).transpose() / 255\n",
    "test_labels = np.array(test_labels[:test_size], dtype = float)\n",
    "train_labels = np.reshape(train_labels, (training_size, 1))\n",
    "test_labels = np.reshape(test_labels, (test_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we want to predict whether a new image is equal to ID or not, we change the labels from train_labels and test_labels to be either 1 if they are equal to ID, or 0 if they are not. This will help us make our predictions later on and calculate the accuracy/error rates. In my case, ID is equal to 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# student ID\n",
    "ID = 6\n",
    "# change the labels to either 0 or 1\n",
    "def relabel(label):\n",
    "    for l in label:\n",
    "        if (l[0] == ID):\n",
    "            l[0] = 1\n",
    "        else:\n",
    "            l[0] = 0\n",
    "            \n",
    "relabel(train_labels)\n",
    "relabel(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We complete preprocessing by creating smaller training sets of the training set for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating smaller data sets of differing size and differing data\n",
    "# subset1 of 10 samples\n",
    "subsetOneImages = train_images[:,:10]\n",
    "ID_labels1 = train_labels[:10,:]\n",
    "\n",
    "# subset2 of 75 samples\n",
    "subsetTwoImages = train_images[:,25:100]\n",
    "ID_labels2 = train_labels[25:100,:]\n",
    "\n",
    "# subset 3 of 1,000 samples\n",
    "subsetThreeImages = train_images[:,200:1200]\n",
    "ID_labels3 = train_labels[200:1200,:]\n",
    "\n",
    "# subset 4 of 10,000 samples\n",
    "subsetFourImages = train_images[:,10000:20000]\n",
    "ID_labels4 = train_labels[10000:20000,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "###### In this section we use the vectorized implementation of the L layer Neural Network, plot the learning curve, and investigate the impact of the learning rate, sample sizes, and the number of hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized implementation of Logistic Gradient Descent with L layers. LGD function will also plot cost curve. The equations used respectively: \n",
    "\n",
    "###### Forward Propogation \n",
    "$z^{[L]} = w^{[L]}A^{[L-1]} + b^{[L]}$\n",
    "\n",
    "$A^{[L]} = tanh(z^{[L]})$\n",
    "\n",
    "$A^{[L]} = ReLU(z^{[L]}) $ (used for the last layer)\n",
    "\n",
    "###### Backward Propogation \n",
    "$dZ^{[L]} = dA^{[L]} * g^{[L]'}(Z^{[L]}) $\n",
    "\n",
    "$dW^{[L]} = {1 \\over m} dZ^{[L]} A^{[L-1]T} $\n",
    "\n",
    "$dB^{[L]} = {1 \\over m} sum horizontally(dZ^{[L]}) $\n",
    "\n",
    "$dA^{[L-1]} = w^{[L]T} dZ^{[L]} $\n",
    "\n",
    "###### Cost Function\n",
    "$J(w,b) = -{1 \\over m} \\sum\\limits_{1=1}^{m} (y log(a[L-1]) + (1 - y)log(1 - a[L-1]) $\n",
    "###### Update Parameters\n",
    "\n",
    "$ w = w - \\alpha dW $\n",
    "\n",
    "$ b = b - \\alpha dB $\n",
    "\n",
    "where $\\alpha$ is the learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU (array):\n",
    "    return np.maximum(array,0)\n",
    "    \n",
    "def dReLU(array):\n",
    "    return (array >= 0).astype(np.float64)\n",
    "\n",
    "def LGD(samples, X, y, alpha, layer_list):\n",
    "    \n",
    "    \n",
    "    \n",
    "    l = len(layer_list)\n",
    "    m = X.shape[1]\n",
    "    costs = []\n",
    "    w  = [0] * (l)\n",
    "    b  = [0] * (l)\n",
    "    z  = [0] * (l)\n",
    "    a  = [0] * (l)\n",
    "    dZ = [0] * (l)\n",
    "    dW = [0] * (l)\n",
    "    dB = [0] * (l)\n",
    "    dA = [0] * (l)\n",
    "    #weight initialization\n",
    "    for i in range(1, l):\n",
    "        w[i] = np.random.randn(layer_list[i], layer_list[i - 1]) / 1000\n",
    "        b[i] = np.random.randn(layer_list[i], 1) / 1000\n",
    "        z[i] = np.zeros((layer_list[i], m))\n",
    "        a[i] = np.zeros((layer_list[i], m))\n",
    "        dZ[i] = np.zeros((layer_list[i], m))\n",
    "        dW[i] = np.zeros((layer_list[i], layer_list[i - 1]))\n",
    "        dB[i] = np.zeros((layer_list[i], 1))\n",
    "        dA[i] = np.zeros((layer_list[i], m))\n",
    "    dA[0] = np.zeros((layer_list[0], m))\n",
    "    a[0] = X \n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(0, 500):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # forward propopgation\n",
    "        for j in range (1, l):\n",
    "            z[j] = (w[j] @ a[j-1]) + b[j]\n",
    "            a[j] = ReLU(z[j])\n",
    "        a[l-1] = expit(z[l-1]) #sigmoid activation function for last layer\n",
    "\n",
    "        \n",
    "        \n",
    "        #compute cost\n",
    "        cost = 0\n",
    "        cost = (np.nan_to_num(\n",
    "            -y @ np.log(a[l-1]) - (1 - y) @ np.log(1 - a[l-1]))[0, 0] / m)\n",
    "        costs.append(cost)\n",
    "\n",
    "        \n",
    "        \n",
    "        #backward propogation \n",
    "        dA[-1] = - (y.T @ np.log(a[-1].T) + (1 - y.T) @ np.log(1 - a[-1].T))\n",
    "        for k in range(l-1, 0, -1):\n",
    "            dZ[k] = dA[k] * dReLU(z[k])\n",
    "            dW[k] = dZ[k] @ a[k-1].transpose() / m      \n",
    "            dB[k] = dZ[k].sum(axis = 1).reshape(dB[k].shape[0], dB[k].shape[1]) / m\n",
    "            dA[k - 1] = w[k].T @ dZ[k]\n",
    "\n",
    "            \n",
    "            \n",
    "        #update parameters \n",
    "        for j in range (1, l):\n",
    "            w[j] = w[j] - (alpha * dW[j])\n",
    "            b[j] = b[j] - (alpha * dB[j])\n",
    "            \n",
    "\n",
    "            \n",
    "    # plot cost function\n",
    "    plt.plot(costs)\n",
    "    plt.title(\"Student ID Cost Function (\" + str(samples) + \" samples)\")\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    return(w, b)\n",
    "\n",
    "\n",
    "def checkAccuracy(w,b) :\n",
    "    a = test_images\n",
    "    z = None\n",
    "\n",
    "    # forward propopgation \n",
    "    for j in range (1, len(layers)):\n",
    "        z = w[j] @ a + b[j]\n",
    "        a = ReLU(z)\n",
    "    a = expit(z)\n",
    "\n",
    "    misclassified = []\n",
    "    count = 0\n",
    "    result = np.where(a >= 0.5, 1.0, 0.0)\n",
    "    for i in range(1,10000):\n",
    "        if result[0, i] == test_labels[i,0]:\n",
    "            count += 1\n",
    "        else:\n",
    "            misclassified.append(i)\n",
    "        \n",
    "    accuracy = (count/test_labels.shape[0]) * 100\n",
    "    error_rate = 100 - accuracy\n",
    "\n",
    "    print(\"accuracy for ID = \" + str(ID) + \" is : \" + str(accuracy) + \"%\")\n",
    "    print(\"error rate for ID = \" + str(ID) + \" is : \" + str(error_rate) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Disclaimer \n",
    "\n",
    "I spent many hours working on and debugging this project, but in the end I could not figure out all of my bugs. Here are the results of my code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [784, 20, 10, 1] #3 layers with 20, 10, and 1 hidden units respectively\n",
    "w,b = LGD(10, subsetOneImages, ID_labels1, .05, layers) #10 samples, alpha = .05\n",
    "checkAccuracy(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [784, 20, 15, 10, 1] #4 layers with 20, 15, 10, 1 hidden units respectively \n",
    "w,b = LGD(10, subsetOneImages, ID_labels1, .5, layers) #10 samples, alpha = .5\n",
    "checkAccuracy(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [784, 20, 10, 1] #3 layers with 20, 10, and 1 hidden units respectively\n",
    "w,b = LGD(75, subsetTwoImages, ID_labels2, .05, layers) #75 samples, alpha = .05\n",
    "checkAccuracy(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [784, 20, 15, 10, 1] #4 layers with 20, 15, 10, and 1 hidden units respectively\n",
    "w,b = LGD(75, subsetTwoImages, ID_labels2, .5, layers) #75 samples, alpha = .5\n",
    "checkAccuracy(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layers = [784, 20, 15, 10, 1] #4 layers with 20, 15, 10 and 1 hidden units respectively\n",
    "w,b = LGD(1000, subsetThreeImages, ID_labels3, .05, layers) #1000 samples, alpha = .05\n",
    "checkAccuracy(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layers = [784, 20, 10, 1] #3 layers with 20, 15, 10, and 1 hidden units respectively\n",
    "w,b = LGD(1000, subsetThreeImages, ID_labels3, .5, layers) #1000 samples, alpha = .5\n",
    "checkAccuracy(w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train the model with all 60,000 samples to get weights w and b respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = [784, 20, 15, 10, 1] #4 layers with 20, 15, 10 and 1 hidden units respectively\n",
    "w, b = LGD(60000, train_images, train_labels, .5, layers) #alpha = .5, hidden units = 10\n",
    "checkAccuracy(w,b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
